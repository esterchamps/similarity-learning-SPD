{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from scipy import ndimage\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'C:\\\\Users\\\\ester\\\\faculdade\\\\tcc\\\\database_gpds\\\\SignatureGPDSSyntheticSignaturesManuscripts\\\\firmasSINTESISmanuscritas\\\\'\n",
    "NUM_OF_WRITERS_TO_LOAD = 10\n",
    "GENUINE_SIGNATURES_PREFIX = 'c-'\n",
    "FORGED_SIGNATURES_PREFIX = 'cf-'\n",
    "TRAINING_SET_RATIO = 0.7\n",
    "VALIDATION_SET_RATIO = 0.2\n",
    "NUM_GENUINE_PER_WRITER = 24\n",
    "NUM_FORGED_PER_WRITER = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grayscale_image(image_path):\n",
    "    '''\n",
    "    Carrega a imagem em escala de cinzas e devolve um ndarray.\n",
    "    '''\n",
    "    image = cv.imread(image_path)\n",
    "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    image_array = np.array(gray_image)\n",
    "\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory_path, num_of_individuals):\n",
    "    data = []\n",
    "    individuals_counter = 0\n",
    "\n",
    "    for subdir, _, files in os.walk(directory_path):\n",
    "        if subdir == directory_path:\n",
    "            continue\n",
    "\n",
    "        individual = os.path.basename(subdir)\n",
    "        genuine_signatures = [\n",
    "            load_grayscale_image(os.path.join(directory_path, subdir, file)) \\\n",
    "            for file in files if file.startswith(GENUINE_SIGNATURES_PREFIX)\n",
    "        ]\n",
    "        forged_signatures = [\n",
    "            load_grayscale_image(os.path.join(directory_path, subdir, file)) \\\n",
    "            for file in files if file.startswith(FORGED_SIGNATURES_PREFIX)\n",
    "        ]\n",
    "        data.append({\n",
    "            'individual': int(individual),\n",
    "            'genuine_signatures': genuine_signatures,\n",
    "            'forged_signatures': forged_signatures\n",
    "        })\n",
    "        individuals_counter += 1\n",
    "        if individuals_counter == num_of_individuals:\n",
    "            break\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data(DATASET_PATH, NUM_OF_WRITERS_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otsu_thresholding(image_array):\n",
    "    '''\n",
    "    Aplica a limiarização de Otsu numa imagem.\n",
    "    Recebe um ndarray.\n",
    "    Retorna um ndarray onde os pixels brancos representam o objeto e os pixels pretos\n",
    "    representam o plano de fundo.\n",
    "    '''\n",
    "    _, otsu_result = cv.threshold(image_array, 0, 1, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "    otsu_inverted = otsu_result^1\n",
    "    return otsu_inverted\n",
    "\n",
    "def thin_operation(img, th_level):\n",
    "    '''\n",
    "    Aplica th_level operações de dilatação e, em seguida, th_level operações de\n",
    "    erosão na imagem img.\n",
    "    Recebe um ndarray contendo uma máscara binária de uma imagem.\n",
    "    Devolve uma máscara binária também, com o resultado da operação.\n",
    "    '''\n",
    "    img = ndimage.binary_opening(img, iterations=th_level)\n",
    "    img = ndimage.binary_erosion(img, iterations=th_level)\n",
    "    return img.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_patch_densities(image):\n",
    "    '''\n",
    "    Calcula as densidades de cada bloco 5x5 centrado em um pixel de assinatura.\n",
    "    A densidade de um bloco é uma fração de quantos pixels do bloco são pixels\n",
    "    de assinatura (pixels brancos, iguais a 1).\n",
    "    Recebe uma máscara binária da assinatura.\n",
    "    Retorna um array com os valores das densidades.\n",
    "    '''\n",
    "    ld = []\n",
    "    pad = 2\n",
    "    padded_image = np.pad(image, pad, mode='constant', constant_values=0)\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            if image[i, j] == 1:\n",
    "                patch = padded_image[i:i+5, j:j+5]\n",
    "                patch_density = np.sum(patch) / 25\n",
    "                ld.append(patch_density)\n",
    "    \n",
    "    if len(ld) == 0:\n",
    "        ld.append(0)\n",
    "    return np.array(ld)\n",
    "\n",
    "def get_optimal_thinning_level(image_array):\n",
    "    '''\n",
    "    Calcula a quantidade ótima de operações de abertura/fechamento na imagem.\n",
    "    Recebe um ndarray com a máscara binária da imagem.\n",
    "    Retorna um inteiro, correspondente ao número ótimo de operações de\n",
    "    abertura/fechamento.\n",
    "    '''\n",
    "    pd = []\n",
    "    th_level = 0\n",
    "    while True:\n",
    "        thinned_image = thin_operation(image_array, th_level)\n",
    "        ld = calculate_patch_densities(thinned_image)\n",
    "        ld_mean = np.mean(ld)\n",
    "        if len(pd) > 1 and np.abs(pd[-1] - ld_mean) < 0.12:\n",
    "            break\n",
    "        pd.append(ld_mean)\n",
    "        th_level += 1\n",
    "    pd_diff = np.abs(np.diff(pd))\n",
    "    otl = np.argmax(pd_diff) + 1\n",
    "    return otl\n",
    "\n",
    "def get_most_optimal_thinning_level(genuine_signatures_of_an_individual_preprocessed_with_ostu):\n",
    "    '''\n",
    "    Retorna a quantidade ótima de operações de abertura/fechamento para o conjunto\n",
    "    de imagens de assinaturas genuínas de um indivíduo.\n",
    "    Recebe a lista de assinaturas genuínas de um único indivíduo.\n",
    "    Retorna um inteiro, correspondente ao número ótimo de operações de abertura/fechamento.\n",
    "    '''\n",
    "    otl = []\n",
    "    index = 0\n",
    "    for signature_image in genuine_signatures_of_an_individual_preprocessed_with_ostu:\n",
    "        signature_otl = get_optimal_thinning_level(signature_image)\n",
    "        otl.append(signature_otl)\n",
    "        index += 1\n",
    "    motl = np.mean(otl)\n",
    "    return motl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(data):\n",
    "    '''\n",
    "    Aplica o pré-processamento nas imagens de assinaturas.\n",
    "    Aplica limiarização de Otsu + abertura/fechamento.\n",
    "    '''\n",
    "    for individual_data in data:\n",
    "        preprocessed_genuine_signatures = []\n",
    "        for sig in individual_data['genuine_signatures']:\n",
    "            otsu_sig = otsu_thresholding(sig)\n",
    "            preprocessed_genuine_signatures.append(otsu_sig)\n",
    "\n",
    "        preprocessed_forged_signatures = []\n",
    "        for sig in individual_data['forged_signatures']:\n",
    "            otsu_sig = otsu_thresholding(sig)\n",
    "            preprocessed_forged_signatures.append(otsu_sig)\n",
    "\n",
    "        motl = get_most_optimal_thinning_level(preprocessed_genuine_signatures)\n",
    "        individual_data['motl'] = round(motl)\n",
    "        individual_data['genuine_signatures_mask'] = \\\n",
    "            list(map(lambda sig: thin_operation(sig, individual_data['motl']), preprocessed_genuine_signatures))\n",
    "        individual_data['forged_signatures_mask'] = \\\n",
    "            list(map(lambda sig: thin_operation(sig, individual_data['motl']), preprocessed_forged_signatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_preprocessing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data_dict):\n",
    "    data_dict.pop('genuine_signatures')\n",
    "    data_dict.pop('forged_signatures')\n",
    "    data_dict.pop('genuine_signatures_mask')\n",
    "    data_dict.pop('forged_signatures_mask')\n",
    "    return data_dict\n",
    "\n",
    "#dataset2 = list(map(transform, dataset.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_map(image, binary_mask):\n",
    "    '''\n",
    "    Obtém o mapa de features da imagem. O mapa consiste na aplicação de\n",
    "    10 funções sob os pixels da imagem: gradientes de primeiro e segundo grau,\n",
    "    magnitude do gradiente, direção do gradiente e normalização dos valores\n",
    "    das coordenadas.\n",
    "    Recebe a imagem (em escala de cinzas) e a máscara binária da assinatura.\n",
    "    O mapa final terá apenas os valores resultantes correspondentes aos píxels\n",
    "    da assinatura.\n",
    "    Retorna um ndarray.\n",
    "    '''\n",
    "    image = image / 255.0\n",
    "    h, w = image.shape\n",
    "    Ix = np.gradient(image, axis=1)\n",
    "    Iy = np.gradient(image, axis=0)\n",
    "    Ixx = np.gradient(Ix, axis=1)\n",
    "    Ixy = np.gradient(Ix, axis=0)\n",
    "    Iyy = np.gradient(Iy, axis=0)\n",
    "    gradient_magnitude = np.sqrt(Ix**2 + Iy**2)\n",
    "    gradient_direction = np.arctan2(Iy, Ix)\n",
    "    xn = np.tile(np.arange(w) / w, (h, 1))\n",
    "    yn = np.tile(np.arange(h) / h, (w, 1)).T\n",
    "\n",
    "    feature_map = np.stack([\n",
    "        image, Ix, Iy, Ixx, Ixy, Iyy, gradient_magnitude, gradient_direction, xn, yn\n",
    "    ], axis=0)\n",
    "    feature_map_only_signature_pixels = feature_map * binary_mask[np.newaxis, :, :]\n",
    "    return feature_map_only_signature_pixels\n",
    "\n",
    "def calculate_signature_covariance_matrix(feature_map):\n",
    "    '''\n",
    "    Calcula a matriz de covariância, dado um mapa de features.\n",
    "    Retorna um ndarray com dimensões (10, 10).\n",
    "    '''\n",
    "    feature_map = feature_map.reshape(10, -1)\n",
    "    S = feature_map.shape[1]\n",
    "    mu = np.mean(feature_map, axis=1, keepdims=True)\n",
    "    inner_summup = (feature_map - mu) @ (feature_map - mu).T\n",
    "    covariance_matrix = (1 / (S - 1)) * inner_summup\n",
    "    return covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signature_covariance_matrices(data):\n",
    "    '''\n",
    "    Calcula as matrizes de covariância para todas as assinaturas do conjunto\n",
    "    de dados carregado.\n",
    "    '''\n",
    "    for individual_data in data:\n",
    "        genuine_signatures_covariance_matrices = []\n",
    "        for signature, signature_mask in zip(individual_data['genuine_signatures'], individual_data['genuine_signatures_mask']):\n",
    "            feature_map = calculate_feature_map(signature, signature_mask)\n",
    "            covariance_matrix = calculate_signature_covariance_matrix(feature_map)\n",
    "            genuine_signatures_covariance_matrices.append(covariance_matrix)\n",
    "\n",
    "        forged_signatures_covariance_matrices = []\n",
    "        for signature, signature_mask in zip(individual_data['forged_signatures'], individual_data['forged_signatures_mask']):\n",
    "            feature_map = calculate_feature_map(signature, signature_mask)\n",
    "            covariance_matrix = calculate_signature_covariance_matrix(feature_map)\n",
    "            forged_signatures_covariance_matrices.append(covariance_matrix)\n",
    "        \n",
    "        individual_data['genuine_scm'] = genuine_signatures_covariance_matrices\n",
    "        individual_data['forged_scm'] = forged_signatures_covariance_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_signature_covariance_matrices(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(data, forgery_type):\n",
    "    similar_pairs_dict = {}\n",
    "    disimilar_pairs_dict = {}\n",
    "\n",
    "    for individual_data in data:\n",
    "        person_number = individual_data['individual']\n",
    "\n",
    "        for i in range(NUM_GENUINE_PER_WRITER):\n",
    "            for j in range(i + 1, NUM_GENUINE_PER_WRITER):\n",
    "                if i != j:\n",
    "                    key = (person_number, person_number)\n",
    "                    value = (i, j)\n",
    "                    similar_pairs_dict.setdefault(key, set()).add(value)\n",
    "\n",
    "        if forgery_type == 'genuine_genuine_mixed_individuals':\n",
    "            for other_d in data:\n",
    "                if other_d['individual'] != person_number and (other_d['individual'], person_number) not in disimilar_pairs_dict:\n",
    "                    for genuine_index in range(NUM_GENUINE_PER_WRITER):\n",
    "                        for other_genuine_index in range(NUM_GENUINE_PER_WRITER):\n",
    "                            key = (person_number, other_d['individual'])\n",
    "                            value = (genuine_index, other_genuine_index)\n",
    "                            disimilar_pairs_dict.setdefault(key, set()).add(value)\n",
    "        \n",
    "        elif forgery_type == 'genuine_forged_mixed_individuals':\n",
    "            for other_d in data:\n",
    "                if other_d['individual'] != person_number:\n",
    "                    for genuine_index in range(NUM_GENUINE_PER_WRITER):\n",
    "                        for forged_index in range(NUM_FORGED_PER_WRITER):\n",
    "                            key = (person_number, other_d['individual'])\n",
    "                            value = (genuine_index, forged_index)\n",
    "                            disimilar_pairs_dict.setdefault(key, set()).add(value)\n",
    "\n",
    "        elif forgery_type == 'genuine_forged_same_individuals':\n",
    "            for genuine_index in range(NUM_GENUINE_PER_WRITER):\n",
    "                for forged_index in range(NUM_FORGED_PER_WRITER):\n",
    "                    key = (person_number, person_number)\n",
    "                    value = (genuine_index, forged_index)\n",
    "                    disimilar_pairs_dict.setdefault(key, set()).add(value)\n",
    "    \n",
    "    return similar_pairs_dict, disimilar_pairs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, forgery_type):\n",
    "\n",
    "    training_individuals = random.sample(data, int(len(data) * TRAINING_SET_RATIO))\n",
    "    remaining_individuals = [d for d in data if d not in training_individuals]\n",
    "    validation_individuals = random.sample(remaining_individuals, int(len(data) * VALIDATION_SET_RATIO))\n",
    "    test_individuals = [d for d in remaining_individuals if d not in validation_individuals]\n",
    "\n",
    "    similar_pairs_training_dict, disimilar_pairs_training_dict = create_dicts(training_individuals, forgery_type)\n",
    "    similar_pairs_validation_dict, disimilar_pairs_validation_dict = create_dicts(validation_individuals, forgery_type)\n",
    "    similar_pairs_test_dict, disimilar_pairs_test_dict = create_dicts(test_individuals, forgery_type)\n",
    "\n",
    "    dicts = [similar_pairs_training_dict, disimilar_pairs_training_dict, similar_pairs_validation_dict, \\\n",
    "             disimilar_pairs_validation_dict, similar_pairs_test_dict, disimilar_pairs_test_dict]\n",
    "\n",
    "    for dictionary in dicts:\n",
    "        for key in dictionary:\n",
    "            dictionary[key] = list(dictionary[key])\n",
    "\n",
    "    return similar_pairs_training_dict, disimilar_pairs_training_dict, \\\n",
    "        similar_pairs_validation_dict, disimilar_pairs_validation_dict, \\\n",
    "        similar_pairs_test_dict, disimilar_pairs_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pairs_training, d_pairs_training, s_pairs_validation, d_pairs_validation, s_pairs_test, d_pairs_test = \\\n",
    "        split_dataset(dataset, 'genuine_forged_mixed_individuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 23)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(s_pairs_training[10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "m = 2\n",
    "p = 5\n",
    "max_iterations = 200\n",
    "learning_rate = 10e-4\n",
    "epoch = 50\n",
    "max_epochs = max_iterations // epoch\n",
    "training_batch_size = 300\n",
    "validation_batch_size = 200\n",
    "zeta_s = 1\n",
    "zeta_d = 20.0\n",
    "sci = 0.001\n",
    "aucs = []\n",
    "minimum_loss = 10e+5\n",
    "best_A = np.zeros((m, 2))\n",
    "best_W = np.zeros((m, 2))\n",
    "best_M = np.zeros((m, m))\n",
    "\n",
    "A = np.random.rand(m, 2)\n",
    "M = np.eye(m)\n",
    "M0 = np.eye(m)\n",
    "W = np.zeros((n, m*p))\n",
    "for j in range(m):\n",
    "    Q = np.random.randn(n, p)\n",
    "    for i in range(j):\n",
    "        Q -= np.dot(W[:, i*p:(i+1)*p], np.dot(W[:, i*p:(i+1)*p].T, Q))\n",
    "    Q, _ = np.linalg.qr(Q)\n",
    "    W[:, j*p:(j+1)*p] = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs_training = s_pairs_training\n",
    "disimilar_pairs_training = d_pairs_training\n",
    "similar_pairs_validating = s_pairs_validation\n",
    "disimilar_pairs_validating = d_pairs_validation\n",
    "similar_pairs_test = s_pairs_test\n",
    "disimilar_pairs_test = d_pairs_test\n",
    "mixed_or_same_individuals_in_similar = 'same'\n",
    "mixed_or_same_individuals_in_disimilar = 'mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(num_elements, source_pairs_dict, source_data, same_or_mixed, origin_list_1, origin_list_2):\n",
    "\n",
    "    list_of_already_selected = []\n",
    "    list_of_pairs = []\n",
    "\n",
    "    if same_or_mixed == 'same':\n",
    "        while len(list_of_pairs) < num_elements:\n",
    "            random_key = random.choice(list(source_pairs_dict.keys()))\n",
    "            pairs = source_pairs_dict[random_key]\n",
    "            pair_index = random.randint(0, len(pairs) - 1)\n",
    "            identifier = (random_key[0], pair_index)\n",
    "\n",
    "            if identifier not in list_of_already_selected:\n",
    "                list_of_already_selected.append(identifier)\n",
    "                pair_of_indexes = pairs[pair_index]\n",
    "                pair = (source_data[random_key[0] - 1][origin_list_1][pair_of_indexes[0]],\n",
    "                        source_data[random_key[0] - 1][origin_list_2][pair_of_indexes[1]])\n",
    "                list_of_pairs.append(pair)\n",
    "\n",
    "    elif same_or_mixed == 'mixed':\n",
    "        while len(list_of_pairs) < num_elements:\n",
    "\n",
    "            random_key = random.choice(list(source_pairs_dict.keys()))\n",
    "            pairs = source_pairs_dict[random_key]\n",
    "            pair_index = random.randint(0, len(pairs) - 1)\n",
    "            identifier = (random_key[0], random_key[1], pair_index)\n",
    "\n",
    "            if identifier not in list_of_already_selected:\n",
    "                list_of_already_selected.append(identifier)\n",
    "                pair_with_indexes = pairs[pair_index]\n",
    "                pair = (source_data[random_key[0] - 1][origin_list_1][pair_with_indexes[0]],\n",
    "                        source_data[random_key[1] - 1][origin_list_2][pair_with_indexes[1]])\n",
    "                list_of_pairs.append(pair)\n",
    "                \n",
    "    return np.array(list_of_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_beta_divergence(matrix1_p_x_p, matrix2_p_x_p, alpha, beta):\n",
    "    '''\n",
    "    Cálculo da subdistância gA(·, ·).\n",
    "    Retorna uma tupla com: valor da divergência/distância, vetor de autovalores e matrix de autovetores.\n",
    "    '''\n",
    "    \n",
    "    inverse2 = np.linalg.inv(matrix2_p_x_p)\n",
    "    m1_dot_inverse2 = matrix1_p_x_p @ inverse2\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(m1_dot_inverse2)\n",
    "    divergence = \\\n",
    "        (1 / (alpha * beta)) \\\n",
    "        * np.sum(np.log(np.divide((alpha * np.power(eigenvalues, beta) + beta * np.power(eigenvalues, -alpha)), (alpha + beta))))\n",
    "\n",
    "    return divergence, eigenvalues, eigenvectors\n",
    "\n",
    "def get_set_of_m_SPD_matrices(covariance_matrix_n_x_n, learnable_parameter_W_n_x_mp, m, p):\n",
    "    '''\n",
    "    Transformação de ponto em conjunto, Ts(·). Gera uma lista com m projeções, fW(·), de menor dimensão.\n",
    "    '''\n",
    "    Zi = np.transpose(learnable_parameter_W_n_x_mp) @ covariance_matrix_n_x_n @ learnable_parameter_W_n_x_mp\n",
    "    rows, cols = Zi.shape\n",
    "    block_diagonal_matrices = []\n",
    "    start_row, start_col = 0, 0\n",
    "\n",
    "    while len(block_diagonal_matrices) < m:\n",
    "        end_row = min(start_row + p, rows)\n",
    "        end_col = min(start_col + p, cols)\n",
    "        block_diagonal_matrix = Zi[start_row:end_row, start_col:end_col]\n",
    "        block_diagonal_matrices.append(block_diagonal_matrix)\n",
    "        start_row += p\n",
    "        start_col += p\n",
    "\n",
    "    return np.array(block_diagonal_matrices)\n",
    "\n",
    "def calculate_point_to_point_distance(learnable_parameter_M_m_x_m, local_distances_vector_Rm):\n",
    "    '''\n",
    "    Distância entre conjuntos, Ds(·, ·). Calculada com a função de integração hM (·).\n",
    "    '''\n",
    "    sum_of_d_M_d_terms = np.einsum('kl,k,l', learnable_parameter_M_m_x_m, local_distances_vector_Rm, local_distances_vector_Rm)\n",
    "    return sum_of_d_M_d_terms\n",
    "\n",
    "def calculate_dij_and_its_eigenvalues_and_eigenvectors(pairs_of_sets, A):\n",
    "    '''\n",
    "    pairs_of_sets -> list of tuples with 2 elements, each with size m.\n",
    "    A -> m x 2\n",
    "    dij_of_sets -> list (same length as pairs_of_sets) of lists with size m.\n",
    "    eigenvalues_of_dij -> list (same length as pairs_of_sets) of lists (length m) of lists (p).\n",
    "    '''\n",
    "    dij_of_sets = []\n",
    "    eigenvalues_of_dij = []\n",
    "    eigenvectors_of_dij = []\n",
    "    for Xi_set, Xj_set in pairs_of_sets:\n",
    "        dij = []\n",
    "        eigenvalues = []\n",
    "        eigenvectors = []\n",
    "        for Xki, Xkj, Ak in zip(Xi_set, Xj_set, A):\n",
    "            alpha, beta = Ak\n",
    "            dkij, lambda_kij, Ukij = get_alpha_beta_divergence(Xki, Xkj, alpha, beta)\n",
    "            dij.append(dkij)\n",
    "            eigenvalues.append(lambda_kij)\n",
    "            eigenvectors.append(Ukij)\n",
    "        \n",
    "        dij_of_sets.append(dij)\n",
    "        eigenvalues_of_dij.append(eigenvalues)\n",
    "        eigenvectors_of_dij.append(eigenvectors)\n",
    "    \n",
    "    return np.array(dij_of_sets), np.array(eigenvalues_of_dij), np.array(eigenvectors_of_dij)\n",
    "\n",
    "def apply_point_to_set_transformation(scm_pairs, W, m, p):\n",
    "\n",
    "    pairs_of_sets = []\n",
    "    for Xi, Xj in scm_pairs:\n",
    "        Xi_set = get_set_of_m_SPD_matrices(Xi, W, m, p)\n",
    "        Xj_set = get_set_of_m_SPD_matrices(Xj, W, m, p)\n",
    "        pairs_of_sets.append((Xi_set, Xj_set))\n",
    "\n",
    "    return np.array(pairs_of_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Dij(list_of_dij, M):\n",
    "    list_of_Dij = []\n",
    "    \n",
    "    for dij in list_of_dij:\n",
    "        Dij = calculate_point_to_point_distance(M, dij)\n",
    "        list_of_Dij.append(Dij)\n",
    "    \n",
    "    return np.array(list_of_Dij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dL_dDij_S(Dij_S, zeta_s):\n",
    "    Dij_minus_zeta_s = Dij_S - zeta_s\n",
    "    Dij_minus_zeta_s[Dij_minus_zeta_s < 0] = 0\n",
    "    dL_dDij = 2 * Dij_minus_zeta_s\n",
    "\n",
    "    return dL_dDij\n",
    "\n",
    "def calculate_dL_dDij_D(Dij_D, zeta_d):\n",
    "    zeta_d_minus_Dij = zeta_d - Dij_D\n",
    "    zeta_d_minus_Dij[zeta_d_minus_Dij < 0] = 0\n",
    "    dL_dDij = -2 * zeta_d_minus_Dij\n",
    "\n",
    "    return dL_dDij\n",
    "\n",
    "def calculate_dL_ddij_S(dij_S, Dij_S, zeta_s, MT_plus_M):\n",
    "    num_pairs, m = dij_S.shape\n",
    "\n",
    "    dL_dDij = calculate_dL_dDij_S(Dij_S, zeta_s)\n",
    "    reshaped_dij_S = dij_S.reshape(num_pairs, 1, m)\n",
    "    reshaped_dL_dDij = dL_dDij[:, np.newaxis, np.newaxis]\n",
    "    dL_dij_S = reshaped_dL_dDij * np.matmul(reshaped_dij_S, MT_plus_M)\n",
    "    return dL_dij_S.reshape(num_pairs, m)\n",
    "\n",
    "def calculate_dL_ddij_D(dij_D, Dij_D, zeta_d, MT_plus_M):\n",
    "    num_pairs, m = dij_D.shape\n",
    "\n",
    "    dL_dDij = calculate_dL_dDij_D(Dij_D, zeta_d)\n",
    "    reshaped_dij_D = dij_D.reshape(num_pairs, 1, m)\n",
    "    reshaped_dL_dDij = dL_dDij[:, np.newaxis, np.newaxis]\n",
    "    dL_dij_D = reshaped_dL_dDij * np.matmul(reshaped_dij_D, MT_plus_M)\n",
    "    return dL_dij_D.reshape(num_pairs, m)\n",
    "\n",
    "def calculate_dL_dA(\n",
    "  S,\n",
    "  D,\n",
    "  eignv_S,\n",
    "  eignv_D,\n",
    "  dij_S, \n",
    "  dij_D, \n",
    "  Dij_S, \n",
    "  Dij_D, \n",
    "  A, \n",
    "  zeta_s, \n",
    "  zeta_d,\n",
    "  MT_plus_M):\n",
    "\n",
    "  \"\"\"\n",
    "  Euclidean gradient of the loss function L with respect to the learnable parameter A.\n",
    "\n",
    "  S, list of genuine, genuine pairs / tuples.\n",
    "  eignv_S -> list (same length as S) of lists (length m) of lists (p).\n",
    "  dij_S -> list (same length as S) of lists with size m.\n",
    "  Dij_S -> list (same length as S).\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def term1_ddkij_dalpha_k(alpha_k, beta_k, lambda_kij):\n",
    "    term1_ddkij_dalpha_k = \\\n",
    "    (alpha_k * np.power(lambda_kij, beta_k) -\n",
    "     alpha_k * beta_k * np.power(lambda_kij, -alpha_k) * np.log(lambda_kij)) / \\\n",
    "    (alpha_k * np.power(lambda_kij, beta_k) + beta_k * np.power(lambda_kij, -alpha_k))\n",
    "\n",
    "    return term1_ddkij_dalpha_k\n",
    "  \n",
    "  def term1_ddkij_dbeta_k(alpha_k, beta_k, lambda_kij):\n",
    "    term1_ddkij_dbeta_k = \\\n",
    "    (beta_k * np.power(lambda_kij, -alpha_k) -\n",
    "     alpha_k * beta_k * np.power(lambda_kij, beta_k) * np.log(lambda_kij))/ \\\n",
    "    (alpha_k * np.power(lambda_kij, beta_k) + beta_k * np.power(lambda_kij, -alpha_k))\n",
    "    \n",
    "    return term1_ddkij_dbeta_k\n",
    "\n",
    "  m, _ = A.shape\n",
    "  _, _, p = eignv_S.shape\n",
    "  dL_dA = np.zeros_like(A)\n",
    "\n",
    "  for k in range(m):\n",
    "    alpha_k, beta_k = A[k]\n",
    "\n",
    "    term2_ddkij_dalpha_k = np.full((p,), alpha_k / (beta_k + alpha_k)) \n",
    "    term2_ddkij_dbeta_k = np.full((p,), beta_k / (beta_k + alpha_k))\n",
    "\n",
    "    dL_dij_S = calculate_dL_ddij_S(dij_S, Dij_S, zeta_s, MT_plus_M)\n",
    "    dL_dij_D = calculate_dL_ddij_D(dij_D, Dij_D, zeta_d, MT_plus_M)\n",
    "\n",
    "    for Dij, dL_dij, lambda_ij in zip(Dij_S, dL_dij_S, eignv_S):\n",
    "\n",
    "      term1_ddkij_dalpha_k_S = term1_ddkij_dalpha_k(alpha_k, beta_k, lambda_ij[k])\n",
    "      term1_ddkij_dbeta_k_S = term1_ddkij_dbeta_k(alpha_k, beta_k, lambda_ij[k])\n",
    "\n",
    "      term3_ddkij_dalpha_or_beta_S = \\\n",
    "      np.log((alpha_k * np.power(lambda_ij[k], beta_k) + beta_k * np.power(lambda_ij[k], -alpha_k)) / \\\n",
    "      (alpha_k + beta_k))\n",
    "\n",
    "      ddkij_dalpha_k_S = (1 / np.power(alpha_k, 2) * beta_k) * (term1_ddkij_dalpha_k_S - term2_ddkij_dalpha_k - term3_ddkij_dalpha_or_beta_S)\n",
    "      ddkij_dbeta_k_S = (1 / alpha_k * np.power(beta_k, 2)) * (term1_ddkij_dbeta_k_S - term2_ddkij_dbeta_k - term3_ddkij_dalpha_or_beta_S)\n",
    "\n",
    "      ddkij_dalpha_k_S = np.sum(ddkij_dalpha_k_S)\n",
    "      ddkij_dbeta_k_S = np.sum(ddkij_dbeta_k_S)\n",
    "\n",
    "      dL_dA[k, 0] += (1 / len(S)) * dL_dij[k] * ddkij_dalpha_k_S\n",
    "      dL_dA[k, 1] += (1 / len(S)) * dL_dij[k] * ddkij_dbeta_k_S\n",
    "  \n",
    "    for Dij, dL_dij, lambda_ij in zip(Dij_D, dL_dij_D, eignv_D):\n",
    "      \n",
    "      term1_ddkij_dalpha_k_D = term1_ddkij_dalpha_k(alpha_k, beta_k, lambda_ij[k])\n",
    "      term1_ddkij_dbeta_k_D = term1_ddkij_dbeta_k(alpha_k, beta_k, lambda_ij[k])\n",
    "\n",
    "      term3_ddkij_dalpha_or_beta_D = \\\n",
    "      np.log((alpha_k * np.power(lambda_ij[k], beta_k) + beta_k * np.power(lambda_ij[k], -alpha_k)) / \\\n",
    "      (alpha_k + beta_k))\n",
    "\n",
    "      ddkij_dalpha_k_D = (1 / np.power(alpha_k, 2) * beta_k) * (term1_ddkij_dalpha_k_D - term2_ddkij_dalpha_k - term3_ddkij_dalpha_or_beta_D)\n",
    "      ddij_dbeta_k_D = (1 / alpha_k * np.power(beta_k, 2)) * (term1_ddkij_dbeta_k_D - term2_ddkij_dbeta_k - term3_ddkij_dalpha_or_beta_D)\n",
    "\n",
    "      ddkij_dalpha_k_D = np.sum(ddkij_dalpha_k_D)\n",
    "      ddij_dbeta_k_D = np.sum(ddij_dbeta_k_D)\n",
    "\n",
    "      dL_dA[k, 0] += (1 / len(D)) * dL_dij[k] * ddkij_dalpha_k_D\n",
    "      dL_dA[k, 1] += (1 / len(D)) * dL_dij[k] * ddij_dbeta_k_D\n",
    "\n",
    "  return np.clip(dL_dA, -150, 150)\n",
    "\n",
    "def calculate_dL_dW(\n",
    "    pairs_S,\n",
    "    pairs_D,\n",
    "    low_dim_sets_S,\n",
    "    low_dim_sets_D,\n",
    "    dij_S,\n",
    "    dij_D,\n",
    "    Dij_S,\n",
    "    Dij_D,\n",
    "    dij_eigenvalues_S,\n",
    "    dij_eigenvalues_D,\n",
    "    dij_eigenvectors_S,\n",
    "    dij_eigenvectors_D,\n",
    "    zeta_s,\n",
    "    zeta_d,\n",
    "    MT_plus_M,\n",
    "    W, A, m, p):\n",
    "\n",
    "    '''\n",
    "    Calculates the Euclidean gradient of L with respect to W.\n",
    "\n",
    "    S, list of genuine, genuine pairs of SPD matrices.\n",
    "    D, list of genuine, forged pairs of SPD matrices.\n",
    "    low_dim_sets_S, list of genuine, genuine pairs of low-dimensional sets of SPD matrices.\n",
    "    low_dim_sets_D, list of genuine, forged pairs of low-dimensional sets of SPD matrices.\n",
    "    '''\n",
    "\n",
    "    dL_dW = np.zeros_like(W)\n",
    "\n",
    "    dL_ddij_S = calculate_dL_ddij_S(dij_S, Dij_S, zeta_s, MT_plus_M)\n",
    "    dL_ddij_D = calculate_dL_ddij_D(dij_D, Dij_D, zeta_d, MT_plus_M)\n",
    "\n",
    "    zipped_collections = \\\n",
    "        zip(pairs_S + pairs_D, \\\n",
    "            low_dim_sets_S + low_dim_sets_D, \\\n",
    "            dij_eigenvalues_S + dij_eigenvalues_D, \\\n",
    "            dij_eigenvectors_S + dij_eigenvectors_D, \\\n",
    "            dL_ddij_S + dL_ddij_D)\n",
    "\n",
    "    for Xi_Xj, Xi_set_Xj_set, lambda_ij, Uij, dL_ddij in zipped_collections:\n",
    "        Xi, Xj = Xi_Xj\n",
    "        Xi_set, Xj_set = Xi_set_Xj_set\n",
    "        \n",
    "        for k in range(m):\n",
    "            alpha_k, beta_k = A[k]\n",
    "\n",
    "            Wk = W[:, k*p : (k+1)*p]\n",
    "\n",
    "            dL_dlambda_kij = \\\n",
    "                dL_ddij[k] * (1 / alpha_k*beta_k) * \\\n",
    "                ((alpha_k*beta_k*np.power(lambda_ij[k], beta_k-1) - alpha_k*beta_k*np.power(lambda_ij[k], -alpha_k-1)) / \\\n",
    "                 (alpha_k*np.power(lambda_ij[k], beta_k) + beta_k*np.power(lambda_ij[k], -alpha_k)))\n",
    "\n",
    "            dL_dsigma_kij = np.diag(dL_dlambda_kij)\n",
    "\n",
    "            transpose_Xki = np.transpose(Xi_set[k])\n",
    "            inverse_transpose_Xki = np.linalg.inv(transpose_Xki)\n",
    "            transpose_Xkj = np.transpose(Xj_set[k])\n",
    "            inverse_transpose_Xkj = np.linalg.inv(transpose_Xkj)\n",
    "\n",
    "            dL_dXki = Uij[k] @ dL_dsigma_kij @ np.transpose(Uij[k]) @ inverse_transpose_Xki\n",
    "            dL_dXkj = -1 * inverse_transpose_Xkj @ transpose_Xki @ Uij[k] @ dL_dsigma_kij @ np.transpose(Uij[k]) @ inverse_transpose_Xkj\n",
    "\n",
    "            dL_dWk = np.transpose(Xi) @ Wk @ dL_dXki + \\\n",
    "                     Xi @ Wk @ np.transpose(dL_dXki) + \\\n",
    "                     np.transpose(Xj) @ Wk @ dL_dXkj + \\\n",
    "                     Xj @ Wk @ np.transpose(dL_dXkj)\n",
    "\n",
    "            dL_dW[:, k*p : (k+1)*p] = dL_dWk\n",
    "    \n",
    "    return dL_dW\n",
    "\n",
    "def calculate_dL_dWR(dL_dW, W):\n",
    "    '''\n",
    "    Calculates the Riemannian gradient of L, with respect to W, from the Euclidean one.\n",
    "    '''\n",
    "    dL_dWR = dL_dW - W @ ((1 / 2) * (np.transpose(W) @ dL_dW + np.transpose(dL_dW) @ W))\n",
    "    return dL_dWR\n",
    "\n",
    "def update_W_parameter(W_tminus1, learning_rate, dL_dWR):\n",
    "    W_t, _ = np.linalg.qr(W_tminus1 - learning_rate*dL_dWR)\n",
    "    return W_t\n",
    "\n",
    "def calculate_dL_dM(S, D, S_dij, D_dij, Dij_S, Dij_D, M, M0, zeta_s, zeta_d, sci):\n",
    "  '''\n",
    "  Euclidean gradient of the loss function with respect to the learnable parameter M.\n",
    "\n",
    "  Receives:\n",
    "  List of dij vectors representing the subdistance measures for D pairs (D_dij);\n",
    "  List of dij vectors representing the subdistance measures for S pairs (S_dij);\n",
    "  List of set-to-set distances for S pairs (distances_S);\n",
    "  '''\n",
    "\n",
    "  m, _ = M.shape\n",
    "  num_pairs, _, _, _ = S.shape\n",
    "\n",
    "  # (50,)\n",
    "  dL_dDij_S = calculate_dL_dDij_S(Dij_S, zeta_s).reshape(num_pairs, 1)\n",
    "  dL_dDij_D = calculate_dL_dDij_D(Dij_D, zeta_d).reshape(num_pairs, 1)\n",
    "\n",
    "  term1 = (1 / len(S)) * np.sum(np.matmul(S_dij.reshape(num_pairs, m, 1), (dL_dDij_S * S_dij).reshape(num_pairs, 1, m)), axis=0)\n",
    "  term2 = (1 / len(D)) * np.sum(np.matmul(D_dij.reshape(num_pairs, m, 1), (dL_dDij_D * D_dij).reshape(num_pairs, 1, m)), axis=0)\n",
    "  term3 = sci * (np.linalg.inv(M0) - np.linalg.inv(M))\n",
    "\n",
    "  dL_dM = term1 + term2 + term3\n",
    "  return dL_dM\n",
    "\n",
    "def calculate_dL_dMR(dL_dM, M):\n",
    "    '''\n",
    "    Calculates the Riemannian gradient of L, with respect to M, from the Euclidean one.\n",
    "    '''\n",
    "    dL_dMR = M @ ((1 / 2) * (dL_dM + dL_dM.T) @ M)\n",
    "    return dL_dMR\n",
    "\n",
    "def update_M_parameter(M_tminus1, learning_rate, dL_dMR):\n",
    "    \n",
    "    eigenvalues_Mtminus1, eigenvectors_Mtminus1 = np.linalg.eig(M_tminus1)\n",
    "\n",
    "    # M_tminus1 = eigenvectors_Mtminus1 @ diag_Mtminus1 @ eigenvectors_Mtminus1.T\n",
    "    eigenvalues_Mtminus1_power_1_by_2 = eigenvalues_Mtminus1 ** (1/2)\n",
    "    eigenvalues_Mtminus1_power_minus_1_by_2 = eigenvalues_Mtminus1 ** (-1/2)\n",
    "    \n",
    "    M_tminus1_squareroot = eigenvectors_Mtminus1 @ np.diag(eigenvalues_Mtminus1_power_1_by_2) @ eigenvectors_Mtminus1.T\n",
    "    M_tminus1_negative_squareroot = eigenvectors_Mtminus1 @ np.diag(eigenvalues_Mtminus1_power_minus_1_by_2) @ eigenvectors_Mtminus1.T\n",
    "\n",
    "    inner_term = -learning_rate * (M_tminus1_negative_squareroot @ dL_dMR @ M_tminus1_negative_squareroot)\n",
    "\n",
    "    eigenvalues_inner, eigenvectors_inner = np.linalg.eig(inner_term)\n",
    "\n",
    "    expm = eigenvectors_inner @ np.diag(np.exp(eigenvalues_inner)) @ eigenvectors_inner.T\n",
    "\n",
    "    Mt = M_tminus1_squareroot @ expm @ M_tminus1_squareroot\n",
    "\n",
    "    return Mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(similar_training_batch, disimilar_training_batch, num_epochs):\n",
    "    s_val = get_batch(validation_batch_size, similar_pairs_validating, dataset, mixed_or_same_individuals_in_similar, 'genuine_scm', 'genuine_scm')\n",
    "    d_val = get_batch(validation_batch_size, disimilar_pairs_validating, dataset, mixed_or_same_individuals_in_disimilar, 'genuine_scm', 'forged_scm')\n",
    "\n",
    "    s_pairs_of_sets_val = apply_point_to_set_transformation(s_val, W, m, p)\n",
    "    d_pairs_of_sets_val = apply_point_to_set_transformation(d_val, W, m, p)\n",
    "\n",
    "    S_dij_val, _, _ = calculate_dij_and_its_eigenvalues_and_eigenvectors(s_pairs_of_sets_val, A)\n",
    "    D_dij_val, _, _ = calculate_dij_and_its_eigenvalues_and_eigenvectors(d_pairs_of_sets_val, A)\n",
    "\n",
    "    S_Dij_val = calculate_Dij(S_dij_val, M)\n",
    "    D_Dij_val = calculate_Dij(D_dij_val, M)\n",
    "    total_val = np.concatenate((S_Dij_val, D_Dij_val))\n",
    "    labels_val = np.concatenate((np.ones_like(S_Dij_val), np.zeros_like(D_Dij_val)))\n",
    "\n",
    "    probabilities = np.zeros_like(total_val, dtype=float)\n",
    "    probabilities[total_val < zeta_s] = 1.0\n",
    "    probabilities[total_val > zeta_d] = 0.0\n",
    "    in_between_mask = (total_val >= zeta_s) & (total_val <= zeta_d)\n",
    "    probabilities[in_between_mask] = 1 - ((total_val[in_between_mask] - zeta_s) / (zeta_d - zeta_s))\n",
    "\n",
    "    #fpr, tpr, thresholds = roc_curve(labels_val, probabilities)\n",
    "    auc = roc_auc_score(labels_val, probabilities)\n",
    "    continue_training = False\n",
    "    aucs.append(auc)\n",
    "\n",
    "    if auc < max(aucs, default=100):\n",
    "        new_similar_training_batch = get_batch(training_batch_size, similar_pairs_training, dataset, 'same', 'genuine_scm', 'genuine_scm')\n",
    "        new_disimilar_training_batch = get_batch(training_batch_size, disimilar_pairs_training, dataset, mixed_or_same_individuals_in_disimilar, 'genuine_scm', 'forged_scm')\n",
    "        continue_training = True\n",
    "        return auc, new_similar_training_batch, new_disimilar_training_batch, continue_training\n",
    "\n",
    "    elif auc >= min(aucs, default=100) and num_epochs == max_epochs:\n",
    "        return auc, similar_training_batch, disimilar_training_batch, continue_training\n",
    "\n",
    "    return auc, similar_training_batch, disimilar_training_batch, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maior AUC: 0.5343375\n"
     ]
    }
   ],
   "source": [
    "s_pairs = get_batch(training_batch_size, similar_pairs_training, dataset, mixed_or_same_individuals_in_similar , 'genuine_scm', 'genuine_scm')\n",
    "d_pairs = get_batch(training_batch_size, disimilar_pairs_training, dataset, mixed_or_same_individuals_in_disimilar , 'genuine_scm', 'forged_scm')\n",
    "\n",
    "aucs = []\n",
    "\n",
    "for t in range(1, max_iterations+1):\n",
    "    s_pairs_of_sets = apply_point_to_set_transformation(s_pairs, W, m, p)\n",
    "    d_pairs_of_sets = apply_point_to_set_transformation(d_pairs, W, m, p)\n",
    "\n",
    "    S_dij, S_dij_eigenvalues, S_dij_eigenvectors = calculate_dij_and_its_eigenvalues_and_eigenvectors(s_pairs_of_sets, A)\n",
    "    D_dij, D_dij_eigenvalues, D_dij_eigenvectors = calculate_dij_and_its_eigenvalues_and_eigenvectors(d_pairs_of_sets, A)\n",
    "\n",
    "    S_Dij = calculate_Dij(S_dij, M)\n",
    "    D_Dij = calculate_Dij(D_dij, M)\n",
    "\n",
    "    MT_plus_M = M.T + M\n",
    "\n",
    "    dL_dA = calculate_dL_dA(s_pairs, d_pairs, S_dij_eigenvalues, D_dij_eigenvalues, S_dij, D_dij, S_Dij, D_Dij, A, zeta_s, zeta_d, MT_plus_M)\n",
    "    A = A - learning_rate * dL_dA\n",
    "\n",
    "    dL_dW = calculate_dL_dW(s_pairs, d_pairs, s_pairs_of_sets, d_pairs_of_sets, S_dij, D_dij, S_Dij, D_Dij, S_dij_eigenvalues, D_dij_eigenvalues, S_dij_eigenvectors, D_dij_eigenvectors, zeta_s, zeta_d, MT_plus_M, W, A, m, p)\n",
    "    dL_dWR = calculate_dL_dWR(dL_dW, W)\n",
    "    W = update_W_parameter(W, learning_rate, dL_dWR)\n",
    "\n",
    "    dL_dM = calculate_dL_dM(s_pairs, d_pairs, S_dij, D_dij, S_Dij, D_Dij, M, M0, zeta_s, zeta_d, sci)\n",
    "    dL_dMR = calculate_dL_dMR(dL_dM, M)\n",
    "    M = update_M_parameter(M, learning_rate, dL_dMR)\n",
    "\n",
    "    if t % epoch == 0:\n",
    "        auc, s_pairs, d_pairs, is_to_continue = \\\n",
    "            run_validation(s_pairs, d_pairs, t // epoch)\n",
    "        \n",
    "        if not is_to_continue:\n",
    "            break\n",
    "\n",
    "        if auc <= min(aucs):\n",
    "            best_A = A.copy()\n",
    "            best_M = M.copy()\n",
    "            best_W = W.copy()\n",
    "\n",
    "print('Maior AUC: '+str(max(aucs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
